L1:


def astar(start_node,stop_node):
    open_set=set(start_node)
    close_set=set()
    g={}
    parent={}
    g[start_node]=0
    parent[start_node]=start_node
    while len(open_set):
        n=None
        for v in open_set:
            if n==None or g[v]+heuristic(v)<g[n]+heuristic(n):
                n=v
        if n==stop_node or graph_node[n]==None:
            pass
        else:
                for(m,weight) in get_neighbours(n):
                    if m not in open_set and m not in close_set:
                        open_set.add(m)
                        parent[m]=n
                        g[m]=g[n]+weight
                    else:
                        if g[m]>g[n]+weight:
                            g[m]=g[n]+weight
                            parent[m]=n             
                    if m in close_set:
                        close_set.remove(m)
                        open_set.add(m)
        if n==None:
            print("Path doesnt exist")
            return None
        if n==stop_node:
            path=[]
            while parent[n]!=n:
                path.append(n)
                n=parent[n]
            path.append(start_node)
            path.reverse()
            print("path found:{}".format(path))
            return path
        open_set.remove(n)
        close_set.add(n)
    print("Path doent exist")
    return None
def get_neighbours(v):
    if v in graph_node:
        return graph_node[v]
    else:
        return None
def heuristic(n):
    H_dist={'S':5,'A':4,'B':5,'E':0}
    return H_dist[n]
graph_node={'S':[('A',1),('B',2)],
            'A':[('E',13)],
            'B':[('E',5)]}
astar('S','E')                            





L2:


class Graph:
    def __init__(self,graph,heuristicNodeList,startNode):
        self.graph=graph
        self.H=heuristicNodeList
        self.start=startNode
        self.parent={}
        self.status={}
        self.solutionGraph={}
    def applyAOStar(self):
        self.aostar(self.start,False)
    def getNeighbors(self,v):
        return self.graph.get(v,'')
    def getstatus(self,v):
        return self.status.get(v,0)
    def setstatus(self,v,val):
        self.status[v]=val
    def getHeuristicNodeValue(self,n):
        return self.H.get(n,0)
    def setHeuristicNodeValue(self,n,value):
        self.H[n]=value
    def printSolution(self):
        print("FOR GRAPH SOLUTION, TRAVERSE GRAPH FROM START NODE:",self.start)
        print("--------")
        print(self.solutionGraph)
        print("---------")
    def computeMinimumCostChildNodes(self,v):
        minimumCost=0
        costToChildNodeListDict={}
        costToChildNodeListDict[minimumCost]=[]
        flag=True
        for nodeInfoTupleList in self.getNeighbors(v):
            cost=0
            nodeList=[]
            for c,weight in nodeInfoTupleList:
                cost=cost+self.getHeuristicNodeValue(c)+weight
                nodeList.append(c)
            if flag==True:
                minimumCost=cost
                costToChildNodeListDict[minimumCost]=nodeList
                flag=False
            else:
                if minimumCost>cost:
                    minimumCost=cost
                    costToChildNodeListDict[minimumCost]=nodeList
        return minimumCost,costToChildNodeListDict[minimumCost]
    def aostar(self,v,backTracking):
        print("HEURISTIC VALUES:",self.H)
        print("SOLUTION GRAPH:",self.solutionGraph)
        print("PROCESSING NODE:",v)
        print("-------------")
        if self.getstatus(v)>=0:
            minimumCost,childNodeList=self.computeMinimumCostChildNodes(v)
            self.setHeuristicNodeValue(v,minimumCost)
            self.setstatus(v,len(childNodeList))
            solved=True
        for childNode in childNodeList:
            self.parent[childNode]=v
            if self.getstatus(childNode)!=-1:
                solved=solved&False
        if solved==True:
            self.setstatus(v,-1)
            self.solutionGraph[v]=childNodeList
        if v!=self.start:
            self.aostar(self.parent[v],True)
        if backTracking==False:
            for childNode in childNodeList:
                self.setstatus(childNode,0)
                self.aostar(childNode,False)
h1={'A':1,'B':6,'C':2,'D':12,'E':2,'F':1,'G':5,'H':7,'I':7,'J':1}
g1={'A':[[('B',1),('C',1)],[('D',1)]],'B':[[('G',1)],[('H',1)]],'C':[[('J',1)]],'D':[[('E',1)],[('F',1)]],'G':[[('I',1)]]}
G1=Graph(g1,h1,'A')
G1.applyAOStar()
G1.printSolution()




L3:


import numpy as np
import pandas as pd
data=pd.read_csv("ws.csv",header=None)
concepts=np.array(data.iloc[:,0:-1])
print("\n Instances are:\n",concepts)
target=np.array(data.iloc[:,-1])
print("\n Target values are:\n",target)
def leam(concepts,target):
    specific_h=concepts[0].copy()
    print("\n Initialization of specific_h and general_h")
    print("Specific boundary:",specific_h)
    general_h=[["?"for i in range(len(specific_h))] for i in range(len(specific_h))]
    print("\n Generic Boundary:",general_h)
    for i,h in  enumerate(concepts):
        print("\n Instance",i+1,"is",h)
        if target[i]=="Yes":
            print("Instance is positive")
            for x in range(len(specific_h)):
                if h[x]!=specific_h[x]:
                    specific_h[x]="?"
                    general_h[x][x]="?"
        if target[i]=="No":
            print("Instance is negative")
            for x in range(len(specific_h)):
                if h[x]!=specific_h[x]:
                    general_h[x][x]=specific_h[x]
                else:
                    general_h[x][x]="?"
        print("Specific boundary after",i+1,"instance is",specific_h)
        print("Generic boundary after",i+1,"instance is",general_h)
    indices=[i for i,val in enumerate(general_h) if val==['?','?','?','?','?','?']]
    for i in indices:
        general_h.remove(['?','?','?','?','?','?'])
    return specific_h,general_h
s_final,g_final=leam(concepts,target)
print("Final specific_h:",s_final,sep="\n")
print("Final general_h:",g_final,sep="\n")


L4:
import math
import csv
def load_csv(filename):
    lines=csv.reader(open(filename,"r"))
    dataset=list(lines)
    headers=dataset.pop(0)
    return dataset,headers
class Node:
    def __init__(self,attribute):
        self.attribute=attribute
        self.children=[]
        self.answer=""
def subtables(data,col,delete):
    dic={}
    coldata=[row[col] for row in data]
    attr= list(set(coldata)) 
    for k in attr:
        dic[k] = []
    for y in range(len(data)): 
        key=data[y][col] 
        if delete: 
            del data[y][col]
        dic [key].append(data[y])
    return attr, dic
def entropy(s):
    attr=list(set(s))
    if len(attr)==1:
        return 0
    counts=[0,0]
    for i in range(2):
        counts[i]=sum([1 for x in s if attr[i]==x])/(len(s)*1.0)
    sums=0
    for cnt in counts:
        sums+=-1*cnt * math.log(cnt,2)
    return sums
def compute_gain(data,col):
    attValues,dic=subtables(data,col,delete=False)
    total_entropy=entropy([row[-1] for row in data])
    for x in range(len(attValues)):
        ratio=len(dic[attValues[x]])/(len(data)*1.0)
        entro=entropy([row[-1] for row in dic[attValues[x]]])
        total_entropy-=ratio*entro
    return total_entropy
def build_tree(data,features):
    lastcol=[row[-1] for row in data]
    if(len(set(lastcol)))==1:
        node=Node(" ")
        node.answer=lastcol[0]
        return node
    n=len(data[0])-1
    gains=[compute_gain(data,col) for col in range(n)]
    split=gains.index(max(gains))
    node=Node(features[split])
    fea=features[:split]+features[split+1:]
    attr,dic=subtables(data,split,delete=True)
    for x in range(len(attr)):
        child = build_tree(dic[attr[x]],fea)
        node.children.append((attr[x],child))
    return node
def print_tree(node,level):
    if node.answer!="":
        print("  "*level,node.answer)
        return
    print("  "*level,node.attribute)
    for value,n in node.children:
        print("  "*(level+1),value)
        print_tree(n,level+2)
def classify(node,x_test,features):
    if node.answer!="":
        print(node.answer)
        return
    pos=features.index(node.attribute)
    for value,n in node.children:
        if x_test[pos]==value:
            classify(n,x_test,features)
dataset,features=load_csv("data3.csv")
node=build_tree(dataset,features)
print("The decision tree for the dataset using ID3 algorithm is")
print_tree(node,0)
testdata,features=load_csv("data3_test.csv")
for xtest in testdata:
    print("The test instance:",xtest)
    print("The predicted label",end=" ")
    classify(node,xtest,features)



L5:


import numpy as np
x=np.array(([2,9],[1,5],[3,6]),dtype=float)
y=np.array(([92],[86],[89]),dtype=float)
x=x/np.amax(x,axis=0)
y=y/100
def sigmoid(x):
    return 1/(1+np.exp(-x))
def sigmoid_grad(x):
    return x*(1-x)
epoch=1000
eta=0.2
input_neurons=2
hidden_neurons=3
output_neurons=1
wh=np.random.uniform(size=(input_neurons,hidden_neurons))
bh=np.random.uniform(size=(1,hidden_neurons))
wout=np.random.uniform(size=(hidden_neurons,output_neurons))
bout=np.random.uniform(size=(1,output_neurons))
for i in range(epoch):
    h_ip=np.dot(x,wh)+bh
    h_act=sigmoid(h_ip)
    o_ip=np.dot(h_act,wout)+bout
    output=sigmoid(o_ip)
    Eo=y-output
    outgrad=sigmoid_grad(output)
    d_output=Eo*outgrad
    Eh=d_output.dot(wout.T)
    hiddengrad=sigmoid_grad(h_act)
    d_hidden=Eh*hiddengrad
    wout+=h_act.T.dot(d_output)*eta
    wh+=x.T.dot(d_hidden)*eta
print("normalized input:\n"+str(x))
print("Actual output:\n"+str(y))
print("Predicted output:\n",output)




L6:


import csv,random,math
import statistics as st
from statistics import stdev
def loadcsv(filename):
    lines=csv.reader(open(filename,"r"));
    dataset=list(lines)
    for i in range(len(dataset)):
        dataset[i]=[float(X)for X in dataset[i]]
    return dataset
def splitdataset(dataset,splitRatio):
    testsize=int(len(dataset)*splitRatio);
    trainset=list(dataset);
    testset=[]
    while(len(testset)<testsize):
        index=random.randrange(len(trainset));
        testset.append(trainset.pop(index))
    return[trainset,testset]
def separateByClass(dataset):
    separated={}
    for i in range(len(dataset)):
        X=dataset[i]
        if(X[-1]not in separated):
            separated[X[-1]]=[]
        separated[X[-1]].append(X)
    return separated
def mean(numbers):
    return sum(numbers)/float(len(numbers))
def stdev(numbers):
    avg=mean(numbers)
    variance=sum([pow(X-avg,2)for X in numbers])/float(len(numbers)-1)
    return math.sqrt(variance)
def compute_mean_std(dataset):
    mean_std=[(st.mean(attribute),st.stdev(attribute))for attribute in zip(*dataset)];
    del mean_std[-1]
    return mean_std
def summarizeByClass(dataset):
    separated=separateByClass(dataset)
    summary={}
    for classValue,instances in separated.items():
        summary[classValue]=compute_mean_std(instances)
    return summary
def estimateProbability(X,mean,stdev):
    exponent=math.exp(-(math.pow(X-mean,2)/(2*math.pow(stdev,2))))
    return(1/(math.sqrt(2*math.pi)*stdev))*exponent
def calculateClassProbabilities(summaries,testVector):
    p={}
    for classValue,classsummaries in summaries.items():
        p[classValue]=1
        for i in range(len(classsummaries)):
            mean,stdev=classsummaries[i]
            X=testVector[i]
            p[classValue]*=estimateProbability(X,mean,stdev);
    return p
def predict(summaries,testVector):
    all_p=calculateClassProbabilities(summaries,testVector)
    bestLabel,bestProb=None,-1
    for Ib1,p in all_p.items():
        if bestLabel is None or p>bestProb:
            bestProb=p
            bestLabel=Ib1
    return bestLabel
def perform_classification(summaries,testset):
    predictions=[]
    for i in range(len(testset)):
        result=predict(summaries,testset[i])
        predictions.append(result)
    return predictions
def getAccuracy(testset,predictions):
    correct=0
    for i in range(len(testset)):
        if testset[i][-1]==predictions[i]:
            correct+=1
    return(correct/float(len(testset)))*100.0
dataset=loadcsv('pima-indians-diabetes.csv');
print('Pima indian diabetes dataset loaded...')
print('Total instances available:',len(dataset))
print('Total attributes present:',len(dataset[0])-1)
print('First five instances of dataset:')
for i in range(5):
    print(i+1,':',dataset[i])
splitRatio=0.2
trainingset,testset=splitdataset(dataset,splitRatio)
print("\n Dataset is split into training and testing set:")
print("Training examples={0}\n Testing examples={1}".format(len(trainingset),len(testset)))
summaries=summarizeByClass(trainingset)
predictions=perform_classification(summaries,testset)
accuracy=getAccuracy(testset,predictions)
print("\n Accuracy of the naive baysian classifier is :",accuracy)



L7:


import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.cluster import KMeans
import numpy as np
import pandas as pd
iris=datasets.load_iris()
x=pd.DataFrame(iris.data)                    
x.columns=['Sepal_length','Sepal_width','Petal_length','Petal_width']
y=pd.DataFrame(iris.target)
y.columns=['Targets']
model=KMeans(n_clusters=3)
model.fit(x)
plt.figure(figsize=(14,14))                     
#colormap=np.array(['m','#10da42','#998087'])
colormap=np.array(['red','lime','black'])
plt.subplot(2,2,1)
plt.scatter(x.Petal_length,x.Petal_width,c=colormap[y.Targets],s=40)
plt.title("Real clustering")
plt.xlabel("Petal length")
plt.ylabel("Petal width")
plt.subplot(2,2,2)
plt.scatter(x.Petal_length,x.Petal_width,c=colormap[model.labels_],s=40)
plt.title("KMeans clustering")
plt.xlabel("Petal length")
plt.ylabel("Petal width")
from sklearn import preprocessing
scalar=preprocessing.StandardScaler()
scalar.fit(x)
xsa=scalar.transform(x)
xs=pd.DataFrame(xsa,columns=x.columns)
from sklearn.mixture import GaussianMixture
gmm=GaussianMixture(n_components=3)
gmm.fit(xs)
gmm_y=gmm.predict(xs)
plt.subplot(2,2,3)
plt.scatter(x.Petal_length,x.Petal_width,c=colormap[gmm_y],s=40)
plt.title("GMM clustering")
plt.xlabel("Petal length")
plt.ylabel("Petal width")
print("Observation:GMM using EM Algorithm based clustering matched the true labels more closely than KMeans")
plt.show()


L8:


from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import datasets
iris=datasets.load_iris()
print("Ïris data set loaded")
x_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.1)
print("Dataset is split into training and testing...")
print("Size of training data and its label",x_train.shape,y_train.shape)
print("Size of testing data and its label",x_test.shape,y_test.shape)
for i in range(len(iris.target_names)):
    print("label",i,"-",str(iris.target_names[i]))
    classifier=KNeighborsClassifier(n_neighbors=1) 
    classifier.fit(x_train,y_train)
    y_pred=classifier.predict(x_test)
print("Results of classification using k-nn with k=1")
for r in range(0,len(x_test)):
    print("Sample:",str(x_test[r]),"Äctual-label:",str(y_test[r]),"Predicted_label:",str(y_pred[r]))
print("Classification Accuracy:",classifier.score(x_test,y_test))



L9:


import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

def kernel(point,xmat,k):
    m,n=np.shape(xmat)
    weights=np.mat(np.eye(m))
    for j in range(m):
        diff=point-X[j]
        weights[j,j]=np.exp(diff*diff.T/(-2.0*k**2))
    return weights

def localWeights(point,xmat,ymat,k):
    wei=kernel(point,xmat,k)
    W=(X.T*(wei*X)).I*(X.T*(wei*ymat.T))
    return W

def localWeightregression(xmat,ymat,k):
    m,n=np.shape(xmat)
    ypred=np.zeros(m)
    for i in range(m):
        ypred[i]=xmat[i]*localWeights(xmat[i],xmat,ymat,k)
    return ypred

def graphplot(X,ypred):
    sortindex=X[:,1].argsort(0)
    xsort=X[sortindex][:,0]
    fig=plt.figure()
    ax=fig.add_subplot(1,1,1)
    ax.scatter(bill,tip,color='green')
    ax.plot(xsort[:,1],ypred[sortindex],color='red',linewidth=4)
    plt.xlabel('Total bill')
    plt.ylabel('Tip')
    plt.show();
    
data=pd.read_csv('data10_tips.csv')
bill=np.array(data.total_bill)
tip=np.array(data.tip)
mbill=np.mat(bill)
mtip=np.mat(tip)
m=np.shape(mbill)[1]
one=np.mat(np.ones(m))
X=np.hstack((one.T,mbill.T))
ypred=localWeightregression(X,mtip,0.5)
graphplot(X,ypred)
